{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: A Mathematical Overview\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm used to minimize some function by moving towards the steepest direction of descent. This steepest direction is represented by the negative of the gradient of the function at the current point.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Given a differentiable function $ f(x) $, the goal is to find $ x $ that minimizes $ f(x) $. The algorithm starts with an initial guess $ x_0 $ and iteratively updates it as:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $ x_k $ is the current point\n",
    "- $ \\nabla f(x_k) $ is the gradient of $ f $ at $ x_k $\n",
    "- $ \\alpha $ is the learning rate\n",
    "- $ x_{k+1} $ is the next point\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "The learning rate $ \\alpha $ determines the step size during each iteration. If $ \\alpha $ is too large, the algorithm might overshoot the minimum. If it's too small, the algorithm will be slow to converge.\n",
    "\n",
    "## Convergence Criteria\n",
    "\n",
    "The algorithm stops when $ \\nabla f(x_k) $ is close to zero or after a set number of iterations.\n",
    "\n",
    "## Types of Gradient Descent\n",
    "\n",
    "1. **Batch Gradient Descent**: Uses all samples for each update.\n",
    "2. **Stochastic Gradient Descent (SGD)**: Uses a single sample for each update.\n",
    "3. **Mini-Batch Gradient Descent**: Uses a subset of samples for each update.\n",
    "\n",
    "## Python Implementation\n",
    "\n",
    "```python\n",
    "def gradient_descent(f, df, x0, alpha=0.01, epochs=1000):\n",
    "    x = x0\n",
    "    for i in range(epochs):\n",
    "        grad = df(x)\n",
    "        x = x - alpha * grad\n",
    "    return x\n",
    "```\n",
    "\n",
    "In the code:\n",
    "\n",
    "- `f` is the function to minimize\n",
    "- `df` is its derivative\n",
    "- `x0` is the initial guess\n",
    "\n",
    "## Applications\n",
    "\n",
    "1. Machine Learning Models\n",
    "2. Neural Networks\n",
    "3. Operations Research\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Gradient descent is a versatile optimization algorithm widely used in machine learning and various engineering applications. Proper tuning of parameters like the learning rate is crucial for effective optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
