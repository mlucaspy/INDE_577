{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "## Dataset: Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class representing a node in a decision tree.\n",
    "class DecisionTreeNode:\n",
    "    \"\"\"\n",
    "    A decision tree node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gini : float\n",
    "        Gini impurity of the node.\n",
    "    num_samples : int\n",
    "        Number of samples at the node.\n",
    "    num_samples_per_class : list\n",
    "        Number of samples per class at the node.\n",
    "    predicted_class : int\n",
    "        Class predicted at the node.\n",
    "    feature_index : int\n",
    "        Index of the feature used for splitting.\n",
    "    threshold : float\n",
    "        Threshold value at the node used for splitting.\n",
    "    left : DecisionTreeNode\n",
    "        Left child node.\n",
    "    right : DecisionTreeNode\n",
    "        Right child node.\n",
    "    \"\"\"\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "# Function to calculate the Gini impurity of a set of labels.\n",
    "def gini(y):\n",
    "    m = len(y)\n",
    "    # Gini impurity formula implementation.\n",
    "    return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
    "\n",
    "# Function to find the best split for the data.\n",
    "def best_split(X, y):\n",
    "    m, n = X.shape\n",
    "    if m <= 1:\n",
    "        return None, None\n",
    "\n",
    "    # Preparing for split calculation.\n",
    "    unique_classes = np.unique(y)\n",
    "    num_classes = len(unique_classes)\n",
    "    class_dict = {c: i for i, c in enumerate(unique_classes)}\n",
    "    num_parent = [np.sum(y == c) for c in unique_classes]\n",
    "    best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "    best_idx, best_thr = None, None\n",
    "\n",
    "    # Iterating over all features to find the best split.\n",
    "    for idx in range(n):\n",
    "        # Sorting data and labels based on current feature.\n",
    "        thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "        num_left = [0] * num_classes\n",
    "        num_right = num_parent.copy()\n",
    "        # Calculating Gini impurity for each possible split.\n",
    "        for i in range(1, m):\n",
    "            c = class_dict[classes[i - 1]]\n",
    "            num_left[c] += 1\n",
    "            num_right[c] -= 1\n",
    "            gini_left = 1.0 - sum((num_left[x] / i) ** 2 for x in range(num_classes))\n",
    "            gini_right = 1.0 - sum((num_right[x] / (m - i)) ** 2 for x in range(num_classes))\n",
    "            gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "            # Skipping equal thresholds.\n",
    "            if thresholds[i] == thresholds[i - 1]:\n",
    "                continue\n",
    "            # Updating best split if a better one is found.\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_idx = idx\n",
    "                best_thr = (thresholds[i] + thresholds[i - 1]) / 2 \n",
    "\n",
    "    return best_idx, best_thr\n",
    "\n",
    "# Function to grow the decision tree recursively.\n",
    "def grow_tree(X, y, depth=0, max_depth=100):\n",
    "    # Counting samples per class and choosing the predicted class.\n",
    "    num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]\n",
    "    predicted_class = np.argmax(num_samples_per_class)\n",
    "    # Creating a new tree node.\n",
    "    node = DecisionTreeNode(\n",
    "        gini=gini(y), \n",
    "        num_samples=len(y), \n",
    "        num_samples_per_class=num_samples_per_class, \n",
    "        predicted_class=predicted_class,\n",
    "    )\n",
    "\n",
    "    # Recursively growing the tree if depth limit is not reached.\n",
    "    if depth < max_depth:\n",
    "        idx, thr = best_split(X, y)\n",
    "        if idx is not None:\n",
    "            # Splitting the dataset based on the best split.\n",
    "            indices_left = X[:, idx] < thr\n",
    "            X_left, y_left = X[indices_left], y[indices_left]\n",
    "            X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "            # Assigning split feature and threshold to the node.\n",
    "            node.feature_index = idx\n",
    "            node.threshold = thr\n",
    "            # Recursively creating left and right children.\n",
    "            node.left = grow_tree(X_left, y_left, depth + 1, max_depth)\n",
    "            node.right = grow_tree(X_right, y_right, depth + 1, max_depth)\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample, node):\n",
    "    while node.left:\n",
    "        if sample[node.feature_index] < node.threshold:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    return node.predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are your data and labels\n",
    "tree = grow_tree(X_train, y_train, max_depth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single prediction\n",
    "prediction = predict(X_test[0], tree)\n",
    "\n",
    "# For all test data\n",
    "predictions = [predict(x, tree) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X9 < 3.82]\n",
      "\t[X2 < 3.0700000000000003]\n",
      "\t\t[X11 < 3.8200000000000003]\n",
      "\t\t\tLEAF 0\n",
      "\t\t\tLEAF 0\n",
      "\t\tLEAF 0\n",
      "\t[X6 < 1.4]\n",
      "\t\tLEAF 0\n",
      "\t\t[X12 < 724.5]\n",
      "\t\t\t[X0 < 13.145]\n",
      "\t\t\t\tLEAF 0\n",
      "\t\t\t\tLEAF 0\n",
      "\t\t\tLEAF 0\n"
     ]
    }
   ],
   "source": [
    "# Show Represenation of Tree\n",
    "def print_tree(node, depth=0):\n",
    "    if node is None:\n",
    "        print(\"{}LEAF {}\".format(\"\\t\" * depth, node))\n",
    "    elif node.left is None and node.right is None:\n",
    "        print(\"{}LEAF {}\".format(\"\\t\" * depth, node.predicted_class))\n",
    "    else:\n",
    "        print(\"{}[X{} < {}]\".format(\"\\t\" * depth, node.feature_index, node.threshold))\n",
    "        print_tree(node.left, depth + 1)\n",
    "        print_tree(node.right, depth + 1)\n",
    "\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [21  0  0]\n",
      " [14  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      1.00      0.52        19\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.35        54\n",
      "   macro avg       0.12      0.33      0.17        54\n",
      "weighted avg       0.12      0.35      0.18        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show Confusion Matrix\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp680",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
